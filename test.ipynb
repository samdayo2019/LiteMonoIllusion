{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from resnet_encoder import ResnetEncoder\n",
    "from depth_decoder import DepthDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENERGY_PER_FLOP = 1e-12\n",
    "TIME_PER_FLOP = 1e-9\n",
    "COMM_BANDWIDTH = 1e10\n",
    "ENERGY_PER_BYTE = 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flops(module, input, output):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        kernel_ops = np.prod(module.kernel_size)\n",
    "        cin = module.in_channels // module.groups\n",
    "        cout = module.out_channels\n",
    "        hout, wout = output.shape[2], output.shape[3]\n",
    "        flops = 2 * kernel_ops * cin * cout * hout * wout\n",
    "        return flops\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        flops = 2 * module.in_features * module.out_features\n",
    "        return flops\n",
    "    elif isinstance(module, (nn.ReLU, nn.ELU, nn.MaxPool2d, nn.AvgPool2d)):\n",
    "        flops = output.numel()\n",
    "        return flops\n",
    "    return 0\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global node_id\n",
    "    flops = compute_flops(module, input, output)\n",
    "    param_shapes = [list(p.shape) for p in module.parameters() if hasattr(module, 'parameters')]\n",
    "    \n",
    "    node = {\n",
    "        \"name\": module.__class__.__name__,\n",
    "        \"id\": node_id,\n",
    "        \"opcode\": type(module).__name__,\n",
    "        \"param_shapes\": param_shapes,  \n",
    "        \"energy\": flops * ENERGY_PER_FLOP,\n",
    "        \"runtime\": flops * TIME_PER_FLOP,\n",
    "        \"flops\": flops,\n",
    "        \"size\": sum(p.numel() * p.element_size() for p in module.parameters() if hasattr(module, 'parameters')),\n",
    "    }\n",
    "    nodes.append(node)\n",
    "\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        tensor_to_node[output] = node_id\n",
    "    elif isinstance(output, (tuple, list)):\n",
    "        for out in output:\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                tensor_to_node[out] = node_id\n",
    "\n",
    "    if isinstance(input, (tuple, list)):\n",
    "        for inp in input:\n",
    "            if isinstance(inp, torch.Tensor) and inp in tensor_to_node:\n",
    "                source_id = tensor_to_node[inp]\n",
    "                if source_id != node_id:\n",
    "                    data_volume = inp.numel() * inp.element_size()\n",
    "                    edge = {\n",
    "                        \"source\": source_id,\n",
    "                        \"destination\": node_id,\n",
    "                        \"shape\": list(inp.shape),\n",
    "                        \"latency\": data_volume / COMM_BANDWIDTH,\n",
    "                        \"energy\": data_volume * ENERGY_PER_BYTE,\n",
    "                        \"size\": data_volume\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "    node_id += 1\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj.tolist()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx.helper as helper\n",
    "import numpy as np\n",
    "\n",
    "def build_onnx_from_json(json_nodes, json_edges):\n",
    "    graph_nodes = []\n",
    "    graph_inputs = []\n",
    "    graph_outputs = []\n",
    "    initializers = []\n",
    "    node_map = {} \n",
    "\n",
    "    for node in json_nodes:\n",
    "        node_id = node['id']\n",
    "        op_type = node['opcode']\n",
    "        node_name = f\"node_{node_id}\"\n",
    "        node_map[node_id] = node_name\n",
    "        input_names = []\n",
    "        for edge in json_edges:\n",
    "            if edge['destination'] == node_id:\n",
    "                input_names.append(f\"node_{edge['source']}_output\")\n",
    "        output_name = f\"{node_name}_output\"\n",
    "\n",
    "        onnx_node = helper.make_node(\n",
    "            op_type=op_type,\n",
    "            inputs=input_names,\n",
    "            outputs=[output_name],\n",
    "            name=node_name\n",
    "        )\n",
    "        graph_nodes.append(onnx_node)\n",
    "\n",
    "        if 'param_shapes' in node and node['param_shapes']:\n",
    "            for idx, shape in enumerate(node['param_shapes']):\n",
    "                param_name = f\"{node_name}_param_{idx}\"\n",
    "                initializer = helper.make_tensor(\n",
    "                    name=param_name,\n",
    "                    data_type=onnx.TensorProto.FLOAT,\n",
    "                    dims=shape,\n",
    "                    vals=np.random.rand(*shape).astype(np.float32).flatten()\n",
    "                )\n",
    "                initializers.append(initializer)\n",
    "\n",
    "    for edge in json_edges:\n",
    "        if edge['source'] not in node_map:  \n",
    "            input_name = f\"node_{edge['source']}_output\"\n",
    "            graph_inputs.append(helper.make_tensor_value_info(\n",
    "                input_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['shape']\n",
    "            ))\n",
    "        if edge['destination'] not in node_map:  \n",
    "            output_name = f\"node_{edge['destination']}_output\"\n",
    "            graph_outputs.append(helper.make_tensor_value_info(\n",
    "                output_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['shape']\n",
    "            ))\n",
    "\n",
    "    graph = helper.make_graph(\n",
    "        nodes=graph_nodes,\n",
    "        name=\"ReconstructedGraph\",\n",
    "        inputs=graph_inputs,\n",
    "        outputs=graph_outputs,\n",
    "        initializer=initializers\n",
    "    )\n",
    "\n",
    "    model = helper.make_model(graph, producer_name=\"json_to_onnx\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "edges = []\n",
    "node_id = 0\n",
    "tensor_to_node = {}\n",
    "\n",
    "num_layers = 18  \n",
    "pretrained = False  \n",
    "num_input_images = 1  \n",
    "encoder = ResnetEncoder(num_layers=num_layers, pretrained=pretrained, num_input_images=num_input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in encoder.encoder.modules():\n",
    "    if len(list(module.children())) == 0:\n",
    "        module.register_forward_hook(hook_fn)\n",
    "\n",
    "input_image = torch.randn(1, num_input_images * 3, 224, 224)\n",
    "encoder(input_image)\n",
    "\n",
    "data = {\"nodes\": [convert_to_serializable(node) for node in nodes], \"edges\": [convert_to_serializable(edge) for edge in edges]}\n",
    "with open('resnet_encoder_graph.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "onnx_model = build_onnx_from_json(nodes, edges)\n",
    "onnx.save(onnx_model, \"reconstructed_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DepthDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "edges = []\n",
    "tensor_to_node = {}\n",
    "node_id = 0\n",
    "\n",
    "num_ch_enc = [64, 64, 128, 256, 512] \n",
    "scales = range(4)\n",
    "depth_decoder = DepthDecoder(num_ch_enc=num_ch_enc, scales=scales, num_output_channels=1, use_skips=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in depth_decoder.modules():\n",
    "    if len(list(module.children())) == 0:  \n",
    "        module.register_forward_hook(hook_fn)\n",
    "\n",
    "input_features = [torch.randn(1, ch, 56, 56) for ch in num_ch_enc]\n",
    "depth_decoder(input_features)\n",
    "\n",
    "data = {\"nodes\": [convert_to_serializable(node) for node in nodes], \"edges\": [convert_to_serializable(edge) for edge in edges]}\n",
    "with open('depth_decoder_graph.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
